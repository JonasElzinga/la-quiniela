{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Price Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to take our first steps in ML with ```scikit-learn```. You are provided with a dataset with cars on sale in CarDekho (a vehicle sale portal from India). Let's see if we can predict the price of a car based on its characteristics!\n",
    "\n",
    "The dataset was extracted from [this Kaggle](https://www.kaggle.com/nehalbirla/vehicle-dataset-from-cardekho). I strongly recommend visiting this Kaggle and explore the notebooks from other data scientists in the world working with this same data. It's always nice to see the work that other people genuinely share. You can always learn from them, the way they analyze the data, the algorithms they use and the plots they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>fuel</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>transmission</th>\n",
       "      <th>owner</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>torque</th>\n",
       "      <th>seats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maruti Swift Dzire VDI</td>\n",
       "      <td>2014</td>\n",
       "      <td>450000</td>\n",
       "      <td>145500</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>23.4 kmpl</td>\n",
       "      <td>1248 CC</td>\n",
       "      <td>74 bhp</td>\n",
       "      <td>190Nm@ 2000rpm</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Skoda Rapid 1.5 TDI Ambition</td>\n",
       "      <td>2014</td>\n",
       "      <td>370000</td>\n",
       "      <td>120000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Second Owner</td>\n",
       "      <td>21.14 kmpl</td>\n",
       "      <td>1498 CC</td>\n",
       "      <td>103.52 bhp</td>\n",
       "      <td>250Nm@ 1500-2500rpm</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Honda City 2017-2020 EXi</td>\n",
       "      <td>2006</td>\n",
       "      <td>158000</td>\n",
       "      <td>140000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Third Owner</td>\n",
       "      <td>17.7 kmpl</td>\n",
       "      <td>1497 CC</td>\n",
       "      <td>78 bhp</td>\n",
       "      <td>12.7@ 2,700(kgm@ rpm)</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hyundai i20 Sportz Diesel</td>\n",
       "      <td>2010</td>\n",
       "      <td>225000</td>\n",
       "      <td>127000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>23.0 kmpl</td>\n",
       "      <td>1396 CC</td>\n",
       "      <td>90 bhp</td>\n",
       "      <td>22.4 kgm at 1750-2750rpm</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maruti Swift VXI BSIII</td>\n",
       "      <td>2007</td>\n",
       "      <td>130000</td>\n",
       "      <td>120000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>16.1 kmpl</td>\n",
       "      <td>1298 CC</td>\n",
       "      <td>88.2 bhp</td>\n",
       "      <td>11.5@ 4,500(kgm@ rpm)</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8123</th>\n",
       "      <td>Hyundai i20 Magna</td>\n",
       "      <td>2013</td>\n",
       "      <td>320000</td>\n",
       "      <td>110000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>18.5 kmpl</td>\n",
       "      <td>1197 CC</td>\n",
       "      <td>82.85 bhp</td>\n",
       "      <td>113.7Nm@ 4000rpm</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8124</th>\n",
       "      <td>Hyundai Verna CRDi SX</td>\n",
       "      <td>2007</td>\n",
       "      <td>135000</td>\n",
       "      <td>119000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Fourth &amp; Above Owner</td>\n",
       "      <td>16.8 kmpl</td>\n",
       "      <td>1493 CC</td>\n",
       "      <td>110 bhp</td>\n",
       "      <td>24@ 1,900-2,750(kgm@ rpm)</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8125</th>\n",
       "      <td>Maruti Swift Dzire ZDi</td>\n",
       "      <td>2009</td>\n",
       "      <td>382000</td>\n",
       "      <td>120000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>19.3 kmpl</td>\n",
       "      <td>1248 CC</td>\n",
       "      <td>73.9 bhp</td>\n",
       "      <td>190Nm@ 2000rpm</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8126</th>\n",
       "      <td>Tata Indigo CR4</td>\n",
       "      <td>2013</td>\n",
       "      <td>290000</td>\n",
       "      <td>25000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>23.57 kmpl</td>\n",
       "      <td>1396 CC</td>\n",
       "      <td>70 bhp</td>\n",
       "      <td>140Nm@ 1800-3000rpm</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8127</th>\n",
       "      <td>Tata Indigo CR4</td>\n",
       "      <td>2013</td>\n",
       "      <td>290000</td>\n",
       "      <td>25000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>23.57 kmpl</td>\n",
       "      <td>1396 CC</td>\n",
       "      <td>70 bhp</td>\n",
       "      <td>140Nm@ 1800-3000rpm</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8128 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              name  year  selling_price  km_driven    fuel  \\\n",
       "0           Maruti Swift Dzire VDI  2014         450000     145500  Diesel   \n",
       "1     Skoda Rapid 1.5 TDI Ambition  2014         370000     120000  Diesel   \n",
       "2         Honda City 2017-2020 EXi  2006         158000     140000  Petrol   \n",
       "3        Hyundai i20 Sportz Diesel  2010         225000     127000  Diesel   \n",
       "4           Maruti Swift VXI BSIII  2007         130000     120000  Petrol   \n",
       "...                            ...   ...            ...        ...     ...   \n",
       "8123             Hyundai i20 Magna  2013         320000     110000  Petrol   \n",
       "8124         Hyundai Verna CRDi SX  2007         135000     119000  Diesel   \n",
       "8125        Maruti Swift Dzire ZDi  2009         382000     120000  Diesel   \n",
       "8126               Tata Indigo CR4  2013         290000      25000  Diesel   \n",
       "8127               Tata Indigo CR4  2013         290000      25000  Diesel   \n",
       "\n",
       "     seller_type transmission                 owner     mileage   engine  \\\n",
       "0     Individual       Manual           First Owner   23.4 kmpl  1248 CC   \n",
       "1     Individual       Manual          Second Owner  21.14 kmpl  1498 CC   \n",
       "2     Individual       Manual           Third Owner   17.7 kmpl  1497 CC   \n",
       "3     Individual       Manual           First Owner   23.0 kmpl  1396 CC   \n",
       "4     Individual       Manual           First Owner   16.1 kmpl  1298 CC   \n",
       "...          ...          ...                   ...         ...      ...   \n",
       "8123  Individual       Manual           First Owner   18.5 kmpl  1197 CC   \n",
       "8124  Individual       Manual  Fourth & Above Owner   16.8 kmpl  1493 CC   \n",
       "8125  Individual       Manual           First Owner   19.3 kmpl  1248 CC   \n",
       "8126  Individual       Manual           First Owner  23.57 kmpl  1396 CC   \n",
       "8127  Individual       Manual           First Owner  23.57 kmpl  1396 CC   \n",
       "\n",
       "       max_power                     torque  seats  \n",
       "0         74 bhp             190Nm@ 2000rpm    5.0  \n",
       "1     103.52 bhp        250Nm@ 1500-2500rpm    5.0  \n",
       "2         78 bhp      12.7@ 2,700(kgm@ rpm)    5.0  \n",
       "3         90 bhp   22.4 kgm at 1750-2750rpm    5.0  \n",
       "4       88.2 bhp      11.5@ 4,500(kgm@ rpm)    5.0  \n",
       "...          ...                        ...    ...  \n",
       "8123   82.85 bhp           113.7Nm@ 4000rpm    5.0  \n",
       "8124     110 bhp  24@ 1,900-2,750(kgm@ rpm)    5.0  \n",
       "8125    73.9 bhp             190Nm@ 2000rpm    5.0  \n",
       "8126      70 bhp        140Nm@ 1800-3000rpm    5.0  \n",
       "8127      70 bhp        140Nm@ 1800-3000rpm    5.0  \n",
       "\n",
       "[8128 rows x 13 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"posts.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three different CSV files in this dataset, but we will only use the one called *Car details v3.csv*. As we can see, in our dataframe we have a set of characteristics of cars (like the kilometers driven, the year of the car, the mileage, etc) and their selling price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name             8128\n",
       "year             8128\n",
       "selling_price    8128\n",
       "km_driven        8128\n",
       "fuel             8128\n",
       "seller_type      8128\n",
       "transmission     8128\n",
       "owner            8128\n",
       "mileage          7907\n",
       "engine           7907\n",
       "max_power        7913\n",
       "torque           7906\n",
       "seats            7907\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always a good start to check for the completeness of our dataset. With the ```count``` function above we can see that there are some missing values, specially in the last columns, but not that many. It's a very complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name              object\n",
       "year               int64\n",
       "selling_price      int64\n",
       "km_driven          int64\n",
       "fuel              object\n",
       "seller_type       object\n",
       "transmission      object\n",
       "owner             object\n",
       "mileage           object\n",
       "engine            object\n",
       "max_power         object\n",
       "torque            object\n",
       "seats            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you already know, the type of each column is inferred in ```read_csv``` method from its values. We can see that in our dataframe most of the columns are *object*'s (strings). Only *year*, *selling_price*, *km_driven* and *seats* are numerical.\n",
    "\n",
    "Obviously, the number of *seats* is an integer value, but ```pandas``` is treating it as a float. Why is that? Well, this is just because of the missing values in this column. In ```pandas``` missing values are represented by ```np.nan``` which is an special object in ```numpy``` of type *float64*. Thus, when a numerical column contains missing values, the column type must be considered alwaya as *float64*. In ML, there's really no difference between integers and floats, so may all the columns be treated as floats and it would work the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing supervised ML, the first step is to establish is which one of our columns is going to be the *target* (that means, the column we want to be able to predict). In our case, we are trying to predict the price of the cars so the target will be *selling_price*. This price is in Indian rupees, though, so maybe we can start by converting it into euros (so it would be easier for us to understand it). Today, the exchange rate (says Google) is 1 rupee = 0.012 euros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"selling_price\"] = df[\"selling_price\"] * 0.012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the second step, we need to establish which columns we are going to use as *features* (that means, the columns we want to be able to predict with). Something that will always be helpful in order to choose the features is taking a look at the correlation matrix. We are specially interested in the correlations of our *target* against the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ee759_row0_col0, #T_ee759_row1_col1, #T_ee759_row2_col2, #T_ee759_row3_col3 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ee759_row0_col1, #T_ee759_row1_col0 {\n",
       "  background-color: #f7a98b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ee759_row0_col2, #T_ee759_row2_col0 {\n",
       "  background-color: #9bbcff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ee759_row0_col3, #T_ee759_row3_col0 {\n",
       "  background-color: #dbdcde;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ee759_row1_col2, #T_ee759_row2_col1 {\n",
       "  background-color: #bcd2f7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ee759_row1_col3, #T_ee759_row3_col1 {\n",
       "  background-color: #e2dad5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ee759_row2_col3, #T_ee759_row3_col2 {\n",
       "  background-color: #f3c7b1;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ee759\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ee759_level0_col0\" class=\"col_heading level0 col0\" >year</th>\n",
       "      <th id=\"T_ee759_level0_col1\" class=\"col_heading level0 col1\" >selling_price</th>\n",
       "      <th id=\"T_ee759_level0_col2\" class=\"col_heading level0 col2\" >km_driven</th>\n",
       "      <th id=\"T_ee759_level0_col3\" class=\"col_heading level0 col3\" >seats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ee759_level0_row0\" class=\"row_heading level0 row0\" >year</th>\n",
       "      <td id=\"T_ee759_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_ee759_row0_col1\" class=\"data row0 col1\" >0.414092</td>\n",
       "      <td id=\"T_ee759_row0_col2\" class=\"data row0 col2\" >-0.418006</td>\n",
       "      <td id=\"T_ee759_row0_col3\" class=\"data row0 col3\" >-0.009144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ee759_level0_row1\" class=\"row_heading level0 row1\" >selling_price</th>\n",
       "      <td id=\"T_ee759_row1_col0\" class=\"data row1 col0\" >0.414092</td>\n",
       "      <td id=\"T_ee759_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "      <td id=\"T_ee759_row1_col2\" class=\"data row1 col2\" >-0.225534</td>\n",
       "      <td id=\"T_ee759_row1_col3\" class=\"data row1 col3\" >0.041358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ee759_level0_row2\" class=\"row_heading level0 row2\" >km_driven</th>\n",
       "      <td id=\"T_ee759_row2_col0\" class=\"data row2 col0\" >-0.418006</td>\n",
       "      <td id=\"T_ee759_row2_col1\" class=\"data row2 col1\" >-0.225534</td>\n",
       "      <td id=\"T_ee759_row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
       "      <td id=\"T_ee759_row2_col3\" class=\"data row2 col3\" >0.227336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ee759_level0_row3\" class=\"row_heading level0 row3\" >seats</th>\n",
       "      <td id=\"T_ee759_row3_col0\" class=\"data row3 col0\" >-0.009144</td>\n",
       "      <td id=\"T_ee759_row3_col1\" class=\"data row3 col1\" >0.041358</td>\n",
       "      <td id=\"T_ee759_row3_col2\" class=\"data row3 col2\" >0.227336</td>\n",
       "      <td id=\"T_ee759_row3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x71ac46b2d400>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr(numeric_only=True).style.background_gradient(\"coolwarm\", vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that our target (*selling_price*) has a strong positive correlation with *year* and a not-so-strong-but-still-good negative correlation with *km_driven*. On the other hand, there is no correlation with *seats*. That does makes sense, as we know that the newer the car is, the more it's worth, and the less kilometers it has been driven, the better. Also, we know that the number of seats it's not something relevant to the price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix gives us a hint of the columns that can be useful for a model. Keep in mind, though, that this matrix is only showing us direct linear correlations, so it's not a universal truth. Some variables can be very useful to a ML model because of a non-linear correlation or because of cross-correlations with other variables.\n",
    "\n",
    "The best criteria to select the features to our model is common sense and knowledge of the problem you are solving. In this case we **know** that *seats* can't be a relevant feature, and that's why we shouldn't use it in our model, becasue it won't give us anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"selling_price\"\n",
    "features = [\"year\", \"km_driven\"]\n",
    "X, y = df[features], df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is splitting the data in *train* and *test* sets. The *train* data is the data that we are going to use to fit our model, whilst the *test* data is the data that we will use test our model and see how good it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (6502, 2)\n",
      "X_test shape: (1626, 2)\n",
      "y_train shape: (6502,)\n",
      "y_test shape: (1626,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to perform the train-test split is through the ```train_test_split``` method in ```sklearn```. In the cell above we took 20% of our data to test (and 80% to train). The test size you should take is something for you to decide. The bigger the train set, the better your model will learn, but the bigger the test set is, the more solid your metrics will be. It depends on the volume of data you have, but typically taking 10% or 20% for testing is good enough.\n",
    "\n",
    "This train-test split method from ```sklearn``` is dividing **randomly** your data. Imagine your data is sorted by selling price and you split in the order your data is: your model will be trained with cars with low selling price and you will be testing with cars with a high price!! That's why it is important to randomly split your data. The fourth argument ```random_state``` fixes the random seed to use so you can reexecute the line and get the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our first ML model with the simplest algorithm there is in the world: Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LinearRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just instantiate the model and fit with the training data. With the ```fit``` method, the ```linear_model``` adjusted its parameters to have the minimal squared error against the target. In a linear model, you know there are only a few parameters, the coefficient for each feature and the intercept. You can check them out if you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 9.27105050e+02, -1.56799094e-02]), np.float64(-1858159.5572970135))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.coef_, linear_model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is just telling us that our model will estimate the price of a car with this formula:\n",
    "\n",
    "$$ \\hat{selling\\_price} = -1,858,199.5572 + 927.10505 * year - 0.0156799094 * km\\_driven $$\n",
    "\n",
    "Now ```linear_model``` is a ready to predict and we can try it out with our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>y_real</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>err</th>\n",
       "      <th>%_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>2005</td>\n",
       "      <td>80000</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>-568</td>\n",
       "      <td>3568.0</td>\n",
       "      <td>118.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7778</th>\n",
       "      <td>2017</td>\n",
       "      <td>45000</td>\n",
       "      <td>15384.0</td>\n",
       "      <td>11105</td>\n",
       "      <td>4279.0</td>\n",
       "      <td>27.814613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>2019</td>\n",
       "      <td>60000</td>\n",
       "      <td>8400.0</td>\n",
       "      <td>12724</td>\n",
       "      <td>-4324.0</td>\n",
       "      <td>-51.476190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6630</th>\n",
       "      <td>2020</td>\n",
       "      <td>15000</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>14357</td>\n",
       "      <td>-8357.0</td>\n",
       "      <td>-139.283333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2016</td>\n",
       "      <td>100000</td>\n",
       "      <td>3240.0</td>\n",
       "      <td>9316</td>\n",
       "      <td>-6076.0</td>\n",
       "      <td>-187.530864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3697</th>\n",
       "      <td>2010</td>\n",
       "      <td>120000</td>\n",
       "      <td>2604.0</td>\n",
       "      <td>3440</td>\n",
       "      <td>-836.0</td>\n",
       "      <td>-32.104455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>2012</td>\n",
       "      <td>110000</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>5451</td>\n",
       "      <td>-1251.0</td>\n",
       "      <td>-29.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2009</td>\n",
       "      <td>55500</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>3524</td>\n",
       "      <td>-1424.0</td>\n",
       "      <td>-67.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4535</th>\n",
       "      <td>2015</td>\n",
       "      <td>15000</td>\n",
       "      <td>9600.0</td>\n",
       "      <td>9721</td>\n",
       "      <td>-121.0</td>\n",
       "      <td>-1.260417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>2018</td>\n",
       "      <td>25000</td>\n",
       "      <td>10800.0</td>\n",
       "      <td>12346</td>\n",
       "      <td>-1546.0</td>\n",
       "      <td>-14.314815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1626 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year  km_driven   y_real  y_pred     err       %_err\n",
       "1392  2005      80000   3000.0    -568  3568.0  118.933333\n",
       "7778  2017      45000  15384.0   11105  4279.0   27.814613\n",
       "3727  2019      60000   8400.0   12724 -4324.0  -51.476190\n",
       "6630  2020      15000   6000.0   14357 -8357.0 -139.283333\n",
       "103   2016     100000   3240.0    9316 -6076.0 -187.530864\n",
       "...    ...        ...      ...     ...     ...         ...\n",
       "3697  2010     120000   2604.0    3440  -836.0  -32.104455\n",
       "3001  2012     110000   4200.0    5451 -1251.0  -29.785714\n",
       "94    2009      55500   2100.0    3524 -1424.0  -67.809524\n",
       "4535  2015      15000   9600.0    9721  -121.0   -1.260417\n",
       "1610  2018      25000  10800.0   12346 -1546.0  -14.314815\n",
       "\n",
       "[1626 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_y_pred = linear_model.predict(X_test)\n",
    "\n",
    "results_df = X_test.copy()\n",
    "results_df[\"y_real\"] = y_test\n",
    "results_df[\"y_pred\"] = linear_y_pred.astype(int)\n",
    "results_df[\"err\"] = results_df[\"y_real\"] - results_df[\"y_pred\"]\n",
    "results_df[\"%_err\"] = results_df[\"err\"] / results_df[\"y_real\"] * 100\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above show us the results on the test data. You can see here with just a glance that the results are horrible. There's even cars whose predicted selling price is negative!! It's quite a crappy model, but don't be harsh. It's just a linear regression after all, with only *year* and *km_driven* as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to numerically tell how good or bad a model is you have to use *metrics*. There are many of them, and they depend on the type of problem you are solving. For instance, in a binary classification problem you would use ROC AUC, while in regression problems (like the one we are doing here) you would typically go for RMSE. All these metrics are available in ```sklearn.metrics```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 7698.548290468255\n",
      "MAPE: 0.8988796385823179\n",
      "R^2: 0.16323632984102243\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "print(f\"RMSE: {mean_squared_error(y_test, linear_y_pred)**0.5}\")\n",
    "print(f\"MAPE: {mean_absolute_percentage_error(y_test, linear_y_pred)}\")\n",
    "print(f\"R^2: {r2_score(y_test, linear_y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE tells us about the average error in our predictions: we are failing by 7698â‚¬ on average. The MAPE tells us about the average percentage error, since 7698â‚¬ is not the same in a car priced at 200kâ‚¬ than in a 5kâ‚¬ car: in our case we are failing by 90% on average. Finally the R^2 score is the Pearson's coefficient of predictions against reality: 1 would be a perfect model, while 0 is worse than a monkey predicting the price through what the tarot cards tell him."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the right model for the problem you have to solve is important. Although linear models are simple and great and they have been a very important milestone in scientific history, today there are much more sophisticated algorithms. Today, the state of the art for tabular data (both for regression and classification) are *Gradient Boosting Machines*. In very simple words, they are like a chain of decision trees where each one estimates the error from the previous one.\n",
    "\n",
    "There are many implementations of GBM, and ```sklearn``` has its own. Although for a professional case I would recommend using specific libraries like [LightGBM](https://lightgbm.readthedocs.io/en/v3.3.2/), [XGBoost](https://xgboost.readthedocs.io/en/stable/) or [CatBoost](https://catboost.ai/), for the purposes of this course the ```sklearn```'s implementation ir more than enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>y_real</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>err</th>\n",
       "      <th>%_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>2005</td>\n",
       "      <td>80000</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>1601</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>46.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7778</th>\n",
       "      <td>2017</td>\n",
       "      <td>45000</td>\n",
       "      <td>15384.0</td>\n",
       "      <td>21436</td>\n",
       "      <td>-6052.0</td>\n",
       "      <td>-39.339574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>2019</td>\n",
       "      <td>60000</td>\n",
       "      <td>8400.0</td>\n",
       "      <td>9176</td>\n",
       "      <td>-776.0</td>\n",
       "      <td>-9.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6630</th>\n",
       "      <td>2020</td>\n",
       "      <td>15000</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>7161</td>\n",
       "      <td>-1161.0</td>\n",
       "      <td>-19.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2016</td>\n",
       "      <td>100000</td>\n",
       "      <td>3240.0</td>\n",
       "      <td>8468</td>\n",
       "      <td>-5228.0</td>\n",
       "      <td>-161.358025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3697</th>\n",
       "      <td>2010</td>\n",
       "      <td>120000</td>\n",
       "      <td>2604.0</td>\n",
       "      <td>3314</td>\n",
       "      <td>-710.0</td>\n",
       "      <td>-27.265745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>2012</td>\n",
       "      <td>110000</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>4250</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>-1.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2009</td>\n",
       "      <td>55500</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2586</td>\n",
       "      <td>-486.0</td>\n",
       "      <td>-23.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4535</th>\n",
       "      <td>2015</td>\n",
       "      <td>15000</td>\n",
       "      <td>9600.0</td>\n",
       "      <td>5592</td>\n",
       "      <td>4008.0</td>\n",
       "      <td>41.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>2018</td>\n",
       "      <td>25000</td>\n",
       "      <td>10800.0</td>\n",
       "      <td>9634</td>\n",
       "      <td>1166.0</td>\n",
       "      <td>10.796296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1626 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year  km_driven   y_real  y_pred     err       %_err\n",
       "1392  2005      80000   3000.0    1601  1399.0   46.633333\n",
       "7778  2017      45000  15384.0   21436 -6052.0  -39.339574\n",
       "3727  2019      60000   8400.0    9176  -776.0   -9.238095\n",
       "6630  2020      15000   6000.0    7161 -1161.0  -19.350000\n",
       "103   2016     100000   3240.0    8468 -5228.0 -161.358025\n",
       "...    ...        ...      ...     ...     ...         ...\n",
       "3697  2010     120000   2604.0    3314  -710.0  -27.265745\n",
       "3001  2012     110000   4200.0    4250   -50.0   -1.190476\n",
       "94    2009      55500   2100.0    2586  -486.0  -23.142857\n",
       "4535  2015      15000   9600.0    5592  4008.0   41.750000\n",
       "1610  2018      25000  10800.0    9634  1166.0   10.796296\n",
       "\n",
       "[1626 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbm_model = GradientBoostingRegressor()\n",
    "gbm_model.fit(X_train, y_train)\n",
    "gbm_y_pred = gbm_model.predict(X_test)\n",
    "\n",
    "results_df = X_test.copy()\n",
    "results_df[\"y_real\"] = y_test\n",
    "results_df[\"y_pred\"] = gbm_y_pred.astype(int)\n",
    "results_df[\"err\"] = results_df[\"y_real\"] - results_df[\"y_pred\"]\n",
    "results_df[\"%_err\"] = results_df[\"err\"] / results_df[\"y_real\"] * 100\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the results are better now (at least there are no negative predictions). Let's compare the metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 5892.946433522567\n",
      "MAPE: 0.564421830935943\n",
      "R^2: 0.5097132841642201\n"
     ]
    }
   ],
   "source": [
    "print(f\"RMSE: {mean_squared_error(y_test, gbm_y_pred)**0.5}\")\n",
    "print(f\"MAPE: {mean_absolute_percentage_error(y_test, gbm_y_pred)}\")\n",
    "print(f\"R^2: {r2_score(y_test, gbm_y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to what we saw before, the metrics are all remarkably better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GBM model we have just created is still far from perfect. But, would you be able to better predict the price of a car with just *year* and *km_driven*?\n",
    "\n",
    "Our model is lacking from features. As a general rule, the more (and better) features the better the model is going to perform. You can go for the most sophisticated algorithm in the world, but if you don't have good features you won't be able to predict nothing. That's the key role of the data scientist: find and create good features for your model. This process is commonly referred as *feature engineering*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the truth is that our dataset contains many other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>fuel</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>transmission</th>\n",
       "      <th>owner</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>torque</th>\n",
       "      <th>seats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maruti Swift Dzire VDI</td>\n",
       "      <td>2014</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>145500</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>23.4 kmpl</td>\n",
       "      <td>1248 CC</td>\n",
       "      <td>74 bhp</td>\n",
       "      <td>190Nm@ 2000rpm</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Skoda Rapid 1.5 TDI Ambition</td>\n",
       "      <td>2014</td>\n",
       "      <td>4440.0</td>\n",
       "      <td>120000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Second Owner</td>\n",
       "      <td>21.14 kmpl</td>\n",
       "      <td>1498 CC</td>\n",
       "      <td>103.52 bhp</td>\n",
       "      <td>250Nm@ 1500-2500rpm</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Honda City 2017-2020 EXi</td>\n",
       "      <td>2006</td>\n",
       "      <td>1896.0</td>\n",
       "      <td>140000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Third Owner</td>\n",
       "      <td>17.7 kmpl</td>\n",
       "      <td>1497 CC</td>\n",
       "      <td>78 bhp</td>\n",
       "      <td>12.7@ 2,700(kgm@ rpm)</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hyundai i20 Sportz Diesel</td>\n",
       "      <td>2010</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>127000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>23.0 kmpl</td>\n",
       "      <td>1396 CC</td>\n",
       "      <td>90 bhp</td>\n",
       "      <td>22.4 kgm at 1750-2750rpm</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maruti Swift VXI BSIII</td>\n",
       "      <td>2007</td>\n",
       "      <td>1560.0</td>\n",
       "      <td>120000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>16.1 kmpl</td>\n",
       "      <td>1298 CC</td>\n",
       "      <td>88.2 bhp</td>\n",
       "      <td>11.5@ 4,500(kgm@ rpm)</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8123</th>\n",
       "      <td>Hyundai i20 Magna</td>\n",
       "      <td>2013</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>110000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>18.5 kmpl</td>\n",
       "      <td>1197 CC</td>\n",
       "      <td>82.85 bhp</td>\n",
       "      <td>113.7Nm@ 4000rpm</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8124</th>\n",
       "      <td>Hyundai Verna CRDi SX</td>\n",
       "      <td>2007</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>119000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Fourth &amp; Above Owner</td>\n",
       "      <td>16.8 kmpl</td>\n",
       "      <td>1493 CC</td>\n",
       "      <td>110 bhp</td>\n",
       "      <td>24@ 1,900-2,750(kgm@ rpm)</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8125</th>\n",
       "      <td>Maruti Swift Dzire ZDi</td>\n",
       "      <td>2009</td>\n",
       "      <td>4584.0</td>\n",
       "      <td>120000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>19.3 kmpl</td>\n",
       "      <td>1248 CC</td>\n",
       "      <td>73.9 bhp</td>\n",
       "      <td>190Nm@ 2000rpm</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8126</th>\n",
       "      <td>Tata Indigo CR4</td>\n",
       "      <td>2013</td>\n",
       "      <td>3480.0</td>\n",
       "      <td>25000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>23.57 kmpl</td>\n",
       "      <td>1396 CC</td>\n",
       "      <td>70 bhp</td>\n",
       "      <td>140Nm@ 1800-3000rpm</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8127</th>\n",
       "      <td>Tata Indigo CR4</td>\n",
       "      <td>2013</td>\n",
       "      <td>3480.0</td>\n",
       "      <td>25000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>23.57 kmpl</td>\n",
       "      <td>1396 CC</td>\n",
       "      <td>70 bhp</td>\n",
       "      <td>140Nm@ 1800-3000rpm</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8128 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              name  year  selling_price  km_driven    fuel  \\\n",
       "0           Maruti Swift Dzire VDI  2014         5400.0     145500  Diesel   \n",
       "1     Skoda Rapid 1.5 TDI Ambition  2014         4440.0     120000  Diesel   \n",
       "2         Honda City 2017-2020 EXi  2006         1896.0     140000  Petrol   \n",
       "3        Hyundai i20 Sportz Diesel  2010         2700.0     127000  Diesel   \n",
       "4           Maruti Swift VXI BSIII  2007         1560.0     120000  Petrol   \n",
       "...                            ...   ...            ...        ...     ...   \n",
       "8123             Hyundai i20 Magna  2013         3840.0     110000  Petrol   \n",
       "8124         Hyundai Verna CRDi SX  2007         1620.0     119000  Diesel   \n",
       "8125        Maruti Swift Dzire ZDi  2009         4584.0     120000  Diesel   \n",
       "8126               Tata Indigo CR4  2013         3480.0      25000  Diesel   \n",
       "8127               Tata Indigo CR4  2013         3480.0      25000  Diesel   \n",
       "\n",
       "     seller_type transmission                 owner     mileage   engine  \\\n",
       "0     Individual       Manual           First Owner   23.4 kmpl  1248 CC   \n",
       "1     Individual       Manual          Second Owner  21.14 kmpl  1498 CC   \n",
       "2     Individual       Manual           Third Owner   17.7 kmpl  1497 CC   \n",
       "3     Individual       Manual           First Owner   23.0 kmpl  1396 CC   \n",
       "4     Individual       Manual           First Owner   16.1 kmpl  1298 CC   \n",
       "...          ...          ...                   ...         ...      ...   \n",
       "8123  Individual       Manual           First Owner   18.5 kmpl  1197 CC   \n",
       "8124  Individual       Manual  Fourth & Above Owner   16.8 kmpl  1493 CC   \n",
       "8125  Individual       Manual           First Owner   19.3 kmpl  1248 CC   \n",
       "8126  Individual       Manual           First Owner  23.57 kmpl  1396 CC   \n",
       "8127  Individual       Manual           First Owner  23.57 kmpl  1396 CC   \n",
       "\n",
       "       max_power                     torque  seats  \n",
       "0         74 bhp             190Nm@ 2000rpm    5.0  \n",
       "1     103.52 bhp        250Nm@ 1500-2500rpm    5.0  \n",
       "2         78 bhp      12.7@ 2,700(kgm@ rpm)    5.0  \n",
       "3         90 bhp   22.4 kgm at 1750-2750rpm    5.0  \n",
       "4       88.2 bhp      11.5@ 4,500(kgm@ rpm)    5.0  \n",
       "...          ...                        ...    ...  \n",
       "8123   82.85 bhp           113.7Nm@ 4000rpm    5.0  \n",
       "8124     110 bhp  24@ 1,900-2,750(kgm@ rpm)    5.0  \n",
       "8125    73.9 bhp             190Nm@ 2000rpm    5.0  \n",
       "8126      70 bhp        140Nm@ 1800-3000rpm    5.0  \n",
       "8127      70 bhp        140Nm@ 1800-3000rpm    5.0  \n",
       "\n",
       "[8128 rows x 13 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing is that we cannot pass to the algorithm any non-numerical information, and most of our columns are strings. Can we do something about it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, first of all, we have columns with strings in the form \"X units\". There is a number that we must be able to extract somehow. Check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       23.40\n",
       "1       21.14\n",
       "2       17.70\n",
       "3       23.00\n",
       "4       16.10\n",
       "        ...  \n",
       "8123    18.50\n",
       "8124    16.80\n",
       "8125    19.30\n",
       "8126    23.57\n",
       "8127    23.57\n",
       "Name: mileage, Length: 8128, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"mileage\"].str.split().str[0].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same applies to *engine* and *max_power*. Let's create columns for these as floats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'bhp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmileage_asfloat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmileage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit()\u001b[38;5;241m.\u001b[39mstr[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m      2\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_asfloat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit()\u001b[38;5;241m.\u001b[39mstr[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_power_asfloat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_power\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/researchandinnovation/.venv/lib/python3.12/site-packages/pandas/core/generic.py:6643\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6637\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   6638\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   6639\u001b[0m     ]\n\u001b[1;32m   6641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6642\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6643\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6644\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   6645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/PycharmProjects/researchandinnovation/.venv/lib/python3.12/site-packages/pandas/core/internals/managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/researchandinnovation/.venv/lib/python3.12/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/PycharmProjects/researchandinnovation/.venv/lib/python3.12/site-packages/pandas/core/internals/blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/researchandinnovation/.venv/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/PycharmProjects/researchandinnovation/.venv/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/PycharmProjects/researchandinnovation/.venv/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:133\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'bhp'"
     ]
    }
   ],
   "source": [
    "df[\"mileage_asfloat\"] = df[\"mileage\"].str.split().str[0].astype(float)\n",
    "df[\"engine_asfloat\"] = df[\"engine\"].str.split().str[0].astype(float)\n",
    "df[\"max_power_asfloat\"] = df[\"max_power\"].str.split().str[0].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see Python complaining about converting 'bhp' to a float. If you dig around a little bit, you'll see that the problem is that there is a row in our dataset with *max_power* equal to ' bhp'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"max_power\"] == \" bhp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's something weird. But these things happen, you'll see that data is not perfect and many times (if not always) you need to first make some cleaning. That's just life, guys. Get used to it.\n",
    "\n",
    "In this case I'm just going to put a NaN in this field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df.loc[df[\"max_power\"] == \" bhp\", \"max_power\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this should go ok:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"max_power_asfloat\"] = df[\"max_power\"].str.split().str[0].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our dataframe has 3 more numerical columns that we can use in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"mileage_asfloat\", \"engine_asfloat\", \"max_power_asfloat\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are still some other columns that we can convert into numerical. I'm talking about *fuel*, *seller_type*, *transmission* and *owner*. These are all what we call *categorical columns*. There are a few possible values and all the individuals belong to one of these. These categorical columns can also be converted into numbers assigning a number to each category. See how can we do it in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"fuel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"fuel\"].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is true, though, that using this type of encoding can be a little bit misleading. These numbers are assigned \"randomly\" and there is no relation between the order of the numbers and the characteristics of the categories. In the example above 'CNG' was given the 0, 'Diesel' is 1, 'LPG' is 2 and 'Petrol' is 3: is really 'CNG' closer to 'Diesel' than to 'Petrol'?\n",
    "\n",
    "Some purists prefer encoding with tools like [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?highlight=onehot#sklearn.preprocessing.OneHotEncoder) but I don't want to get into that right now. If you use a tree-model like GBM, the values of the codes is not very very important (I don't say it is not at all). So, for today, we will do this thing to all categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"fuel_encoded\"] = df[\"fuel\"].astype(\"category\").cat.codes\n",
    "df[\"seller_type_encoded\"] = df[\"seller_type\"].astype(\"category\").cat.codes\n",
    "df[\"transmission_encoded\"] = df[\"transmission\"].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column *owner*, however, is worth to be treated specially. Although it is a categorical column, the values of these categories do have a clear order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"owner\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a case like this, you maybe want to put the codes manually like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"owner_encoded\"] = df[\"owner\"].map({\n",
    "    'Test Drive Car': 0,\n",
    "    'First Owner': 1,\n",
    "    'Second Owner': 2,\n",
    "    'Third Owner': 3,\n",
    "    'Fourth & Above Owner': 4,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we now have many numerical columns in our dataset. Let's see the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr(numeric_only=True).style.background_gradient(\"coolwarm\", vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of the new columns present a good correlation with our target, like *max_power_asfloat* or *transmission_encoded*. Let's try now a new GBM with all these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"year\",\n",
    "    \"km_driven\",\n",
    "    \"seats\",\n",
    "    \"engine_asfloat\",\n",
    "    \"max_power_asfloat\",\n",
    "    \"mileage_asfloat\",\n",
    "    \"fuel_encoded\",\n",
    "    \"seller_type_encoded\",\n",
    "    \"transmission_encoded\",\n",
    "    \"owner_encoded\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before that, however, we must first do something about missings values. Models in ```sklearn``` cannot handle NaN values, and thus we have to get rid of them. This problem did not came up before because  ```year``` and ```km_driven``` were complete columns, but that's not the case for many of the new columns we've created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are mainly two options about NaN's. First option is just drop any rows containing missing values in the features selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second option is to fill the NaN values with something. You can fill NaNs with a fixed value like 0 or an atypical value like -99999 so the model can learn these are something special, or you can fill them with the mean value of the corresponding column, so the model will assume \"normality\" for the missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(-9999)  # This fills all NaNs with -9999\n",
    "df.fillna(df.mean(numeric_only=True))  # This fills NaNs with the mean in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, since the number of rows with missing values is not that much, we will use ```dropna```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df.dropna(subset=features)\n",
    "X = _df[features]\n",
    "y = _df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train and test our GBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_model = GradientBoostingRegressor()\n",
    "gbm_model.fit(X_train, y_train)\n",
    "gbm_y_pred = gbm_model.predict(X_test)\n",
    "\n",
    "print(f\"RMSE: {mean_squared_error(y_test, gbm_y_pred)**0.5}\")\n",
    "print(f\"MAPE: {mean_absolute_percentage_error(y_test, gbm_y_pred)}\")\n",
    "print(f\"R^2: {r2_score(y_test, gbm_y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can see that the results now are way way better. Now this is a pretty good model!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters are the config parameters of an algorithm. As opposed to ML model internal parameters, these parameters cannot be adjusted to your data through the ```fit``` method, and it's something that the data scientists must decide prior to train.\n",
    "\n",
    "For instance, in a GBM model, one hyperparameter is the number of trees or the depth of these trees. Each model has its own hyperparameters that you must know in order to properly adjust. Check the documentation of the algorithm you are using to know more about them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These hyperparameters are typically set at model instantiation. Let's run the same model as before but changing some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_model = GradientBoostingRegressor(n_estimators=200, max_depth=6, random_state=15)\n",
    "gbm_model.fit(X_train, y_train)\n",
    "gbm_y_pred = gbm_model.predict(X_test)\n",
    "\n",
    "print(f\"RMSE: {mean_squared_error(y_test, gbm_y_pred)**0.5}\")\n",
    "print(f\"MAPE: {mean_absolute_percentage_error(y_test, gbm_y_pred)}\")\n",
    "print(f\"R^2: {r2_score(y_test, gbm_y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results have improved! In this case we have increased the number of trees (default is 100) and making them more deep (default depth is 3). These two hyperparameters will increase the complexity of the model, increasing also the training time. You should know, though, that more complexity will not always mean more better performance. When a model gets too complex (specially decisions trees) overfitting is a issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_gbm_model = GradientBoostingRegressor(n_estimators=300, max_depth=10, random_state=15)\n",
    "_gbm_model.fit(X_train, y_train)\n",
    "_gbm_y_pred = _gbm_model.predict(X_test)\n",
    "\n",
    "print(f\"RMSE: {mean_squared_error(y_test, _gbm_y_pred)**0.5}\")\n",
    "print(f\"MAPE: {mean_absolute_percentage_error(y_test, _gbm_y_pred)}\")\n",
    "print(f\"R^2: {r2_score(y_test, _gbm_y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no universal rules to decide the proper hyperparameters for a model. Only when you are a real expert (much more than I am) you develop the clinical eye to know what hyperparameters will be the best.\n",
    "\n",
    "The vast majority of people just go with the default or perform something called *grid search*. *Grid search* is just a fancy name for *trial and error*: you choose an array of different values of each hyperparameter, create a grid with all different combinations, try them all and keep the model with best metrics. I'm not a big fan of grid search because when you do that your are using the test data to adjust your model, which is something *philosophically incorrect*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots about model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to metrics, plots are also a great tool to see how your model performs. Also as with metrics, the kind of plots that you may want to draw depends on the kind of problem you are solving. For binary classifications, the ROC curve and Precision-Recall plots cannot be missing. For regressions problems like the one we are dealing with today, the essential plot is *predictions* vs *reality*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16, 8)\n",
    "ax.set_title(\"Model performance\")\n",
    "ax.set_ylabel(\"Pred value\")\n",
    "ax.set_xlabel(\"Real value\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.scatter(y_test, gbm_y_pred, s=1)\n",
    "plot_range = [y_test.min(), y_test.max()]\n",
    "ax.plot(plot_range, plot_range, c=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the more the points of this graph fit the line, the better the model will be. Sometimes with this plot you can also see *where* your model fails the most. For instance, in our model we can see that that the points are further from the line at low values of the real target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another plot that I like to draw is the target and the prediction distributions to see if the model is capturing the global behaviour of the target.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16, 8)\n",
    "ax.set_title(\"Target distributions\")\n",
    "ax.set_ylabel(\"Pred value\")\n",
    "ax.set_xlabel(\"Real value\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.hist(y_test, bins=50, color=\"blue\", alpha=0.5, label=\"Real\")\n",
    "ax.hist(gbm_y_pred, bins=50, color=\"orange\", alpha=0.5, label=\"Prediction\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the error's distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16, 8)\n",
    "ax.set_title(\"Model error distribution\")\n",
    "ax.set_ylabel(\"Number of cases\")\n",
    "ax.set_xlabel(\"Error\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.hist(y_test - gbm_y_pred, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model will never be perfect, that is a reality. Metrics will tell you how good is your model but always in a global way. In order to improve your model, it is important that you try to understand where your model is failing and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = df.loc[y_test.index].copy()\n",
    "preds_df[\"y_real\"] = preds_df[target]  # I'm just adding this column to have both y_real and y_pred at the right of my df\n",
    "preds_df[\"y_pred\"] = gbm_y_pred\n",
    "preds_df[\"err\"] = preds_df[\"y_real\"] - preds_df[\"y_pred\"]\n",
    "preds_df[\"abs_err\"] = preds_df[\"err\"].abs()\n",
    "preds_df[\"%_err\"] = preds_df[\"err\"] / preds_df[\"y_real\"] * 100\n",
    "preds_df[\"abs_%_err\"] = preds_df[\"%_err\"].abs()\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df.sort_values(\"abs_err\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now at this point is where the *business knowledge* comes in. *Business knowledge* is consultant jargon (I'm a consultant), we refer this way to the part of the problem that our client knows more. I don't know nothing about cars, I already told you, but I could be working as a data scientist consultant for CarDekho (why not?) and at this point I would send them this table for them to tell me if this data is consistent or not.\n",
    "\n",
    "Is it logical that Skoda Kodiaq from 2018 (2 years old, this dataset was extracted in 2020) and with this characteristics was priced at 33,600â‚¬? Or does it make more sense that its price was around 13k? To me, ignorant, that sounds expensive. If its correct that the price should be 33k, then why? Is there any extra information that we are missing? Did this car belong to Michael Jackson or something like that?\n",
    "\n",
    "Same questions apply to all other cars with excessive errors. With the proper answer, we may find some other information that can be relevant, add it to the model, and get even better metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting thing when analysing errors is dividing into groups. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df.groupby(\"seller_type\")[[\"y_real\", \"y_pred\", \"abs_err\", \"abs_%_err\"]].agg([\"mean\", \"std\", \"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, I don't know about this topic, and I have literally no idea what a *Trustmark Dealer* is, but it is evident that we are nailing these cars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or maybe we can go for something more interesting like the car brand (assuming the car brand is the first word in the car name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df[\"brand\"] = preds_df[\"name\"].str.split().str[0]\n",
    "error_by_brand = preds_df.groupby(\"brand\")[[\"y_real\", \"y_pred\", \"abs_err\", \"abs_%_err\"]].agg([\"mean\", \"std\", \"count\"])\n",
    "error_by_brand.sort_values((\"abs_%_err\", \"mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_by_brand.loc[\n",
    "    error_by_brand[(\"abs_%_err\", \"count\")] > 5\n",
    "][(\"abs_%_err\", \"mean\")].sort_values(ascending=False).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do fail a lot in Skoda, that's weird, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another part that cannot be missing in a ML model is feature importance analysis. In order to understand the model that you have just created, you have to know which where the most relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of ML algorithms provide some *way* to tell about the importance of the features. Sometimes it's something super clear like in linear models, where the importance of course will be directly related to the coefficients of each feature; sometimes it can be a little more difficult to tell, like in GBM's where, since the result comes from decisions trees, it's not clear what's the specific contribution of each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the GBM implementation from ```sklearn``` we have this attribute that gives us what is called the *Gini importance*. This is a measure of the importance of each feature based on the number of splits (across all trees) that include this feature. As said, that is just a *type of measure* for importance, although is not always clear what *importance* means in this context. As always, a plot is nicer to see the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(gbm_model.feature_importances_, index=features)\n",
    "importances.sort_values(ascending=True).plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we see in here is that *max_power* is clearly the most important feature in our data to predict the price, followed by *year*. It's remarkable how low the importance of *km_driven* is (in my ignorant opinion). The features at the bottom of this chart show no importance and probably the model would work exactly the same if we do not consider them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common measure of importance is *permutation importance*. This is a technique that is appliable to any model (no matter how opaque it is) where the data is tabular. This is calculated for each feature by evaluating the difference in model performance when the feature is randomly shuffled. If a feature is really important, the model performance should decrease a lot when this variable is given random, while, on the other hand, when a feature is really not important, the model should perform the same when the feature is given random.\n",
    "\n",
    "You can import ```permutation_importance``` from ```sklearn``` too. This calculation of features importances can take a little bit longer, but I honestly trust this values a little bit more than Gini importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "imp = permutation_importance(gbm_model, X_test, y_test, n_repeats=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.importances_mean, imp.importances_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(imp.importances_mean, index=features)\n",
    "importances.sort_values(ascending=True).plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are similar, that's expectable, of course. But you can see here that *km_driven* is higher than *engine* and *fuel* has some impact, while *seats* proved to be non-important at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there's this other really new and really cool tool called [shap](https://shap.readthedocs.io/en/latest/index.html). It's a library that aimes to give model explainability with a game theoretic approach and they have some really good results. Remember to install this library before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "shap.initjs()  # Most of shap plots are interactive and require some JS that is loaded with this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(gbm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This operation takes a while\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the cool thing about shap is that it give us the importance (or contribution) that each feature had in each prediction. Individually, not globally. These are the values that we stored in the ```shap_values``` variables. Now, you can ask for how the contributions where for each single prediction. Let's the first one, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot we can see that the feature that was the most important in this prediction was the year of the car (2011, almost ten years old). That pushed down the prediction almost 2kâ‚¬. Secondly, having 73.9 for *max_power* (relatively low) also pushed down a lot. Also, being from fourth owner or above (*owner_encoded* = 4) and having more than 90k kilometers driven pushed down. Being diesel (*fuel_encoded* = 1) pushed it up a little bit, but not so much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other interesting visualizations in shap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary plot shows the importance of all features in all predictions (each point is a prediction). Surely, shap values are giving us another measure of importance. We can plot an importance plot similar to the ones we did before like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a super interesting plot from shap are *dependency plots*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"year\", shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is plotting the shap value (contribution) of *year* feature in each one of our predictions againts the actual value of the *year* feature. In addition, this plot uses color to represent another feature. This other feature is automatically chosen by ```shap``` to be the one with most interaction.\n",
    "\n",
    "First thing to notice here is that the contribution of this feature has an ascending trend only starting in approx 2010; in cars older than this, the contribution is roughly constant (negative). That is telling us that the price will change if the car is from 2015 or 2018, but it is going to be pretty much the same if it is from 2000 or from 1995.\n",
    "\n",
    "Secondly, we can see a special interaction between *year* and *max_power*. It seems that, in recent years (>~2017) the higher the *max_power*, the higher the contribution of this (recent) year. On the other hand, in older years (between ~2005 and ~2015) it seems that the higher the *max_power*, the lower (when we say the lower we mean the higher in negative) the *year* contribution is. The insight is clear: the higher the *max_power* of the car, the more important the *year* becomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this is a very complete example of how to do ML the good way. Remember the main steps:\n",
    "\n",
    "1. Choose the target.\n",
    "2. Analyze the data you have to select the best features.\n",
    "3. Do the train-test split.\n",
    "4. Choose a proper algorithm and train your model.\n",
    "5. Test your model and see the metrics.\n",
    "6. Analyze model errors, features importances and, if you want more, go back to point 2 again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll leave you with the final metrics of our model, just to brag a little bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE: {mean_squared_error(y_test, gbm_y_pred)**0.5}\")\n",
    "print(f\"MAPE: {mean_absolute_percentage_error(y_test, gbm_y_pred)}\")\n",
    "print(f\"R^2: {r2_score(y_test, gbm_y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JFYI, the most voted notebook in this Kaggle scored 0.94, we did 0.977 ðŸ˜‰."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
