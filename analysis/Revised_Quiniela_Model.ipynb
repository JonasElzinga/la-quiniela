{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c64fcbf",
   "metadata": {},
   "source": [
    "# Revised Quiniela Model\n",
    "This notebook aims to improve the prediction model for the 'Quiniela' game by enhancing feature engineering, trying advanced models, and improving the evaluation process. This includes recent team performance metrics, head-to-head data, and advanced model interpretation.\n",
    "\n",
    "Let's dive into the implementation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db434dc2",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing\n",
    "We'll start by loading the dataset and performing initial data preprocessing. This includes handling missing values, encoding categorical data, and preparing features for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe3f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "# Assuming dataset.csv is the data file (replace with actual data file path)\n",
    "data = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Check for missing values and handle them\n",
    "data.fillna(method='ffill', inplace=True)  # Forward fill as a basic approach; customize as needed\n",
    "\n",
    "# Encode categorical features if any\n",
    "# Example: data = pd.get_dummies(data, columns=['Category1', 'Category2'])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = data.drop(columns=['Match_Result'])  # Assuming 'Match_Result' is the target column\n",
    "y = data['Match_Result']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data Preprocessing completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66862c41",
   "metadata": {},
   "source": [
    "## Step 2: Feature Engineering\n",
    "We'll introduce new features to improve model performance, including recent team performance metrics, head-to-head stats, and home/away-specific metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab018ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adding new features (example placeholders)\n",
    "\n",
    "# Recent team performance: average goals scored/conceded over last 5 matches\n",
    "data['Avg_Goals_Scored_Last_5'] = data.groupby('Team')['Goals_Scored'].transform(lambda x: x.rolling(5, 1).mean())\n",
    "data['Avg_Goals_Conceded_Last_5'] = data.groupby('Team')['Goals_Conceded'].transform(lambda x: x.rolling(5, 1).mean())\n",
    "\n",
    "# Head-to-Head performance: Last 5 games between two teams\n",
    "data['Head_to_Head_Last_5'] = data.groupby(['Home_Team', 'Away_Team'])['Match_Result'].transform(lambda x: x.rolling(5, 1).apply(lambda y: sum(y == 'Home Win') / 5, raw=True))\n",
    "\n",
    "# Home/Away specific performance metrics\n",
    "data['Home_Avg_Goals'] = data[data['Home_Team'] == data['Team']]['Goals_Scored'].transform(lambda x: x.rolling(5, 1).mean())\n",
    "data['Away_Avg_Goals'] = data[data['Away_Team'] == data['Team']]['Goals_Scored'].transform(lambda x: x.rolling(5, 1).mean())\n",
    "\n",
    "print(\"Feature Engineering completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c264e",
   "metadata": {},
   "source": [
    "## Step 3: Model Training and Hyperparameter Tuning\n",
    "We'll test models like Random Forest and XGBoost and tune hyperparameters for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e570e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define model and parameters for tuning\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "\n",
    "# Parameter grids\n",
    "param_grid_rf = {'n_estimators': [50, 100, 150], 'max_depth': [10, 20, 30]}\n",
    "param_grid_xgb = {'n_estimators': [50, 100], 'max_depth': [3, 5], 'learning_rate': [0.01, 0.1]}\n",
    "\n",
    "# Grid search for Random Forest\n",
    "grid_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "best_rf = grid_rf.best_estimator_\n",
    "\n",
    "# Grid search for XGBoost\n",
    "grid_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "best_xgb = grid_xgb.best_estimator_\n",
    "\n",
    "print(f\"Best Random Forest Model: {best_rf}\")\n",
    "print(f\"Best XGBoost Model: {best_xgb}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f172bdaa",
   "metadata": {},
   "source": [
    "## Step 4: Model Evaluation and Interpretation\n",
    "We'll evaluate the models using accuracy, confusion matrix, and feature importance. If possible, SHAP values will be used for model interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d749f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predictions using the best models\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "y_pred_xgb = best_xgb.predict(X_test)\n",
    "\n",
    "# Evaluation for Random Forest\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Evaluation for XGBoost\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"XGBoost Classification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# Confusion matrix for the best model\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_xgb)  # using XGBoost as example\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Home Win', 'Draw', 'Away Win'], yticklabels=['Home Win', 'Draw', 'Away Win'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix for XGBoost')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
